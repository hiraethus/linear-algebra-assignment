\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}

\title{Linear Algebra Assignment}
\author{Michael J. Jones}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

\section{Row and Column Picture of a linear model}

\subsection*{Solve the following linear system}

\begin{equation}
2x + y = 7
\end{equation}

\begin{equation}
2x + 3y = 1
\end{equation}

\subsubsection*{Answer}

Given we can express this problem in the form $Ax = b$:

<<>>=
A <- matrix(nrow = 2, data = c(2, 2, 1, 3))
colnames(A) <- c('x', 'y')
rownames(A) <- c('', '')
A

b <- matrix(nrow = 2, ncol = 1, data = c(7, 1))
colnames(b) <- c('')
rownames(b) <- c('', '')
b
@

Knowing that $x = A^{-1} \cdot b$, then

<<>>=
x = solve(A) %*% b
x
@

\subsection*{Draw the row picture of the linear system}

If we express y as a function of x in each of the above equations

\begin{gather}
f(x) = 7 - 2x \\
f(x) = \frac{1 - 2x}{3}
\end{gather}

Then, using the \emph{ggplot2} library, we can plot both of these functions in R.

<<fig=TRUE>>=
library(ggplot2)
library(grid)
# TODO
f.1 <- function(x) 7 - 2*x
f.2 <- function(x) (1 - 2*x) / 3

ggplot(data.frame(x=c(-10, 10)), aes(x)) +
  stat_function(fun=f.1, geom="line", aes(colour="2x + y = 7")) +
  stat_function(fun=f.2, geom="line", aes(colour="2x + 3y = 1")) +
  labs(title="Row Picture", colour="Linear functions") +
  theme(legend.position=c(0.85,0.9))
@

\subsection*{Draw the column picture of the linear system}

\subsubsection*{Answer}
In order to draw our column picture, it helps us to first express our problem as the summation of a set of vectors:

\begin{equation}
\begin{bmatrix}
2 \\
2
\end{bmatrix} x
+
\begin{bmatrix}
1 \\
3
\end{bmatrix} y
=
\begin{bmatrix}
7 \\
1
\end{bmatrix} x
\end{equation}

<<fig=TRUE>>=
ggplot(data.frame(x=c(-5, 10)), aes(x)) +
  geom_segment(colour="blue", aes(x = 0, y = 0, xend = 2, yend = 2), arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(colour="red", aes(x = 0, y = 0, xend = 1, yend = 3), arrow = arrow(length = unit(0.5, "cm"))) +
  labs(title="Vectors") +
  xlab("x") +
  ylab("y") +
  geom_text(vjust=-.2, aes(label=c("v1", "v2")), x = c(1, 2), y = c(3,2))
@

Some combination of $x$ of the first vector and $y$ of the second vector will yield the third vector. Fortunately, 
we have already discovered when we solved this linear system that $x = 5$ and $y = -3$. This means that five of the 
first and negative three of the second vectors would add up to give us $(7, 1)'$.

<<fig=TRUE>>=
ggplot(data.frame(x=c(-5, 10)), aes(x)) +
  scale_x_continuous(limits = c(0, 12)) +
  geom_segment(colour="blue", aes(x = 0, y = 0, xend = 2, yend = 2),
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(colour="red", aes(x = 0, y = 0, xend = 1, yend = 3),
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(linetype="dashed", colour="blue",
               aes(x = 0, y = 0, xend = 2*5, yend = 2*5), 
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(linetype="dashed", colour="red", 
               aes(x = 2*5, y = 2*5, xend = 2*5 - 3*1, yend = 2*5 - 3 * 3),
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(size=1.5, colour="purple", aes(x = 0, y = 0, xend = 7, yend = 1),
               arrow = arrow(length = unit(0.5, "cm"))) +
  labs(title="Column Picture") +
  xlab("x") +
  ylab("y") +
  geom_text(vjust=-1,
            aes(label=c("v1", "v2", "(v2 * 5) + (v1 * -3)","")),
            x = c(1, 2, 7,0),
            y = c(3, 2, 1,0))

@

The column picture plainly shows that the combining v1 and v2 in the proportions of $5$ and $-3$ results in 
observing the vector, $(7,1)'$.

\section{Data}
All data is available at link \url{http://go.qub.ac.uk/toolkit/regularization}

\section{Linear regression model}

\subsection{Question}
Download and read the prostate cancer dataset prostate.data into a data matrix. The
data set is taken from the free online book \emph{The Elements of Statistical Learning} from
Trevor Hastie, Robert Tibshirani and Jerome Friedman. \\
\url{http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data}

\begin{verbatim}
Prostate data info
Predictors (columns 1--8)
lcavol
lweight
age
lbph
svi
lcp
gleason
pgg45

outcome (column 9)

lpsa

train/test indicator (column 10)
\end{verbatim}

\subsubsection*{Answer}

<<results=tex>>=
require(xtable) # install the xtable package if you don't have it
prostate.data.url <-
  "http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data"
prostate.data <- read.table(prostate.data.url, head=T)
xtable(head(prostate.data,3), )
@

\subsection{Question}
Estimate and define a linear model using the $lm()$ function.

\subsubsection*{Answer}

<<print=FALSE>>=
prostate.lm <- lm(formula = lpsa ~ ., data = prostate.data, method = "qr")
@

<<>>=
summary(prostate.lm)
@

\subsection{Question}
Split the prostate dataset into a test and training dataset (see column 10)

\subsubsection*{Answer}

<<results=tex>>=
split.data <- split(prostate.data, prostate.data$train)
prostate.training <- split.data$T
prostate.training <- subset(prostate.training, select=-train)
xtable(head(prostate.training,3), )

prostate.test <- split.data$F
prostate.test <- subset(prostate.test, select=-train)
xtable(head(prostate.test,3), )
@

\subsection{Question}
Predict $lpsa$ for the test example

\begin{verbatim}
data<-read.table(file="...")
# split data
test.data<- ...
train.data<- ...
\end{verbatim}

\subsubsection*{Answer}

First, we will define the linear model again using only the training data.

<<>>=
prostate.lm <- lm(formula = lpsa ~ ., data = prostate.training, method = "qr")
@

We now need to use \verb@predict@ to predict the the lpsa level using our linear model
on our test data. Note, we need to exclude the \verb@lpsa@ column from our test data 
in our prediction.

<<results=tex>>=
prostate.test.no.label <- subset(prostate.test, select=-lpsa)
lm.predicted.lpsa <- predict(prostate.lm, prostate.test.no.label)
@

\subsection{Question}
Plot your results (true $lpsa$ value $y_{i}$ and predicted $\hat{y}_{i}$).

\subsubsection*{Answer}

<<fig=T>>=
lm.true.vs.predicted <- 
  data.frame(true=prostate.test$lpsa,predicted=lm.predicted.lpsa)

ggplot(data = lm.true.vs.predicted, aes(true, predicted)) +
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  ylab(expression(paste("predicted lpsa (", hat(y),')'))) +
  xlab("true lpsa (y)") +
  labs(title="Linear Model of True against Predicted Values")
@
\subsection{Question}
Repeat the analysis using a random forest regression and plot your results (true $lpsa$ value $y_{i}$ and predicted $\hat{y}_{i}$).

\subsubsection*{Answer}

We have already partitioned our data into both training and testing data above. Just as we 
did with linear regression, we need to perform our regression analysis using Random Forests 
with the training data.

<<>>=
library(randomForest)
num.predictors <- 8
prostate.rf <- randomForest(lpsa ~ ., data=prostate.training,
                            mtry=num.predictors / 3, importance=TRUE,
                            na.action=na.omit)

summary(prostate.rf)
@

And as performed above, we can predict the values of our test data for $lpsa$

<<>>=
set.seed(131)
rf.predicted.lpsa <- predict(prostate.rf, prostate.test.no.label)
@

and plot these results accordingly.

<<fig=T>>=
rf.true.vs.predicted <- 
  data.frame(true=prostate.test$lpsa,predicted=rf.predicted.lpsa)

ggplot(data = rf.true.vs.predicted, aes(true, predicted)) +
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  ylab(expression(paste("predicted lpsa (", hat(y),')'))) +
  xlab("true lpsa (y)") +
  labs(title="Random Forests Regression Model of True against Predicted Values")
@

\section{Implement your own lm function}
Write a R function that estimates $\hat{\beta}$ coefficients for the linear model:

\begin{equation}
y = \beta_{0} + \beta_{pred1}x_{1} + \beta_{pred2}x_{2} + \beta_{pred3}x_{3}
\end{equation}

Remember to center scale the predictor variables and estimate $\beta_{0}$ separately. You can 
use the function $scale()$

The coefficients of a linear model can be estimated by

\begin{equation}
\hat{\beta} = \beta_{0} + \beta_{pred1}x_{1} + \beta_{pred2}x_{2} + \beta_{pred3}x_{3}
\end{equation}

\pagebreak
\subsubsection*{Answer}
<<>>=
my.lm <- function(resp, cova){
  errors <- c()
  if (!is.matrix(cova)) errors <- c(errors,  "\ncova must be a matrix")
  if (!is.numeric(cova)) errors <- c(errors, "\ncova must be numeric")
  if (!is.vector(resp)) errors <- c(errors,  "\nresp must be a vector")
  if (!is.numeric(resp)) errors <- c(errors, "\nresp must be numeric")
  if (length(errors) > 0) {
    stop(errors)
  }

  X <- scale(cova,TRUE,TRUE)
  X <- cbind(1,X)
  cp <- t(X) %*% X

  cp.inv <- solve(cp)
  beta <- cp.inv %*%t (X) %*% resp
  
  if (!is.null(colnames(cova))) {
    rows <- colnames(cova)
  } else {
    num.coefs <- ncol(cova)
    rows <- paste("beta", 1:num.coefs, sep=".")
  }
  rownames(beta) <- c("Intercept", rows)
  colnames(beta) <- "coefs"

  return(beta)
}
@

\subsubsection*{An example}
<<results=tex>>=
covariates <- matrix(data=c(2,1,5,4,2,3), nrow=3)
colnames(covariates) <- c("x.1", "x.2")
y <- c(16, 8, 19)

my.lm.coefficients <- my.lm(y, covariates)
xtable(my.lm.coefficients)
@

\end{document}


