\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}

\title{Linear Algebra Assignment}
\author{Michael J. Jones}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

\section{Row and Column Picture of a linear model}

\subsection*{Solve the following linear system}

\begin{equation}
2x + y = 7
\end{equation}

\begin{equation}
2x + 3y = 1
\end{equation}

\subsubsection*{Answer}

Given we can express this problem in the form $Ax = b$:

<<>>=
A <- matrix(nrow = 2, data = c(2, 2, 1, 3))
colnames(A) <- c('x', 'y')
rownames(A) <- c('', '')
A

b <- matrix(nrow = 2, ncol = 1, data = c(7, 1))
colnames(b) <- c('')
rownames(b) <- c('', '')
b
@

Knowing that $x = A^{-1} \cdot b$, then

<<>>=
x = solve(A) %*% b
x
@

\subsection*{Draw the row picture of the linear system}

If we express y as a function of x in each of the above equations

\begin{gather}
f(x) = 7 - 2x \\
f(x) = \frac{1 - 2x}{3}
\end{gather}

Then, using the \emph{ggplot2} library, we can plot both of these functions in R.

<<fig=TRUE>>=
library(ggplot2)
library(grid)
# TODO
f.1 <- function(x) 7 - 2*x
f.2 <- function(x) (1 - 2*x) / 3

ggplot(data.frame(x=c(-10, 10)), aes(x)) +
  stat_function(fun=f.1, geom="line", aes(colour="2x + y = 7")) +
  stat_function(fun=f.2, geom="line", aes(colour="2x + 3y = 1")) +
  labs(title="Row Picture", colour="Linear functions") +
  theme(legend.position=c(0.85,0.9))
@

\subsection*{Draw the column picture of the linear system}

\subsubsection*{Answer}
In order to draw our column picture, it helps us to first express our problem as the summation of a set of vectors:

\begin{equation}
\begin{bmatrix}
2 \\
2
\end{bmatrix} x
+
\begin{bmatrix}
1 \\
3
\end{bmatrix} y
=
\begin{bmatrix}
7 \\
1
\end{bmatrix} x
\end{equation}

<<fig=TRUE>>=
ggplot(data.frame(x=c(-5, 10)), aes(x)) +
  geom_segment(colour="blue", aes(x = 0, y = 0, xend = 2, yend = 2), arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(colour="red", aes(x = 0, y = 0, xend = 1, yend = 3), arrow = arrow(length = unit(0.5, "cm"))) +
  labs(title="Vectors") +
  xlab("x") +
  ylab("y") +
  geom_text(vjust=-.2, aes(label=c("v1", "v2")), x = c(1, 2), y = c(3,2))
@

Some combination of $x$ of the first vector and $y$ of the second vector will yield the third vector. Fortunately, 
we have already discovered when we solved this linear system that $x = 5$ and $y = -3$. This means that five of the 
first and negative three of the second vectors would add up to give us $(7, 1)'$.

<<fig=TRUE>>=
ggplot(data.frame(x=c(-5, 10)), aes(x)) +
  scale_x_continuous(limits = c(0, 12)) +
  geom_segment(colour="blue", aes(x = 0, y = 0, xend = 2, yend = 2),
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(colour="red", aes(x = 0, y = 0, xend = 1, yend = 3),
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(linetype="dashed", colour="blue",
               aes(x = 0, y = 0, xend = 2*5, yend = 2*5), 
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(linetype="dashed", colour="red", 
               aes(x = 2*5, y = 2*5, xend = 2*5 - 3*1, yend = 2*5 - 3 * 3),
               arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(size=1.5, colour="purple", aes(x = 0, y = 0, xend = 7, yend = 1),
               arrow = arrow(length = unit(0.5, "cm"))) +
  labs(title="Column Picture") +
  xlab("x") +
  ylab("y") +
  geom_text(vjust=-1,
            aes(label=c("v1", "v2", "(v2 * 5) + (v1 * -3)","")),
            x = c(1, 2, 7,0),
            y = c(3, 2, 1,0))

@

The column picture plainly shows that the combining v1 and v2 in the proportions of $5$ and $-3$ results in 
observing the vector, $(7,1)'$.

\section{Data}
All data is available at link \url{http://go.qub.ac.uk/toolkit/regularization}

\section{Linear regression model}

\subsection{Question}
Download and read the prostate cancer dataset prostate.data into a data matrix. The
data set is taken from the free online book \emph{The Elements of Statistical Learning} from
Trevor Hastie, Robert Tibshirani and Jerome Friedman. \\
\url{http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data}

\begin{verbatim}
Prostate data info
Predictors (columns 1--8)
lcavol
lweight
age
lbph
svi
lcp
gleason
pgg45

outcome (column 9)

lpsa

train/test indicator (column 10)
\end{verbatim}

\subsubsection*{Answer}

<<results=tex>>=
require(xtable) # install the xtable package if you don't have it
prostate.data.url <-
  "http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data"
prostate.data <- read.table(prostate.data.url, head=T)
xtable(head(prostate.data,3), )
@

\subsection{Question}
Estimate and define a linear model using the $lm()$ function.

\subsubsection*{Answer}

<<print=FALSE>>=
prostate.lm <- lm(formula = lpsa ~ ., data = prostate.data, method = "qr")
@

<<>>=
summary(prostate.lm)
@

\subsection{Question}
Split the prostate dataset into a test and training dataset (see column 10)

\subsubsection*{Answer}

<<results=tex>>=
split.data <- split(prostate.data, prostate.data$train)
prostate.training <- split.data$T
prostate.training <- subset(prostate.training, select=-train)
xtable(head(prostate.training,3), )

prostate.test <- split.data$F
prostate.test <- subset(prostate.test, select=-train)
xtable(head(prostate.test,3), )
@

\subsection{Question}
Predict $lpsa$ for the test example

\begin{verbatim}
data<-read.table(file="...")
# split data
test.data<- ...
train.data<- ...
\end{verbatim}

\subsubsection*{Answer}

First, we will define the linear model again using only the training data.

<<>>=
prostate.lm <- lm(formula = lpsa ~ ., data = prostate.training, method = "qr")
@

We now need to use \verb@predict@ to predict the the lpsa level using our linear model
on our test data. Note, we need to exclude the \verb@lpsa@ column from our test data 
in our prediction.

<<results=tex>>=
prostate.test.no.label <- subset(prostate.test, select=-lpsa)
lm.predicted.lpsa <- predict(prostate.lm, prostate.test.no.label)
@

\subsection{Question}
Plot your results (true $lpsa$ value $y_{i}$ and predicted $\hat{y}_{i}$).

\subsubsection*{Answer}

<<fig=T>>=
lm.true.vs.predicted <- 
  data.frame(true=prostate.test$lpsa,predicted=lm.predicted.lpsa)

ggplot(data = lm.true.vs.predicted, aes(true, predicted)) +
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  ylab(expression(paste("predicted lpsa (", hat(y),')'))) +
  xlab("true lpsa (y)") +
  labs(title="Linear Model of True against Predicted Values")
@
\subsection{Question}
Repeat the analysis using a random forest regression and plot your results (true $lpsa$ value $y_{i}$ and predicted $\hat{y}_{i}$).

\subsubsection*{Answer}

We have already partitioned our data into both training and testing data above. Just as we 
did with linear regression, we need to perform our regression analysis using Random Forests 
with the training data.

<<>>=
library(randomForest)
num.predictors <- 8
prostate.rf <- randomForest(lpsa ~ ., data=prostate.training,
                            mtry=num.predictors / 3, importance=TRUE,
                            na.action=na.omit)

summary(prostate.rf)
@

And as performed above, we can predict the values of our test data for $lpsa$

<<>>=
set.seed(131)
rf.predicted.lpsa <- predict(prostate.rf, prostate.test.no.label)
@

and plot these results accordingly.

<<fig=T>>=
rf.true.vs.predicted <- 
  data.frame(true=prostate.test$lpsa,predicted=rf.predicted.lpsa)

ggplot(data = rf.true.vs.predicted, aes(true, predicted)) +
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  ylab(expression(paste("predicted lpsa (", hat(y),')'))) +
  xlab("true lpsa (y)") +
  labs(title="Random Forests Regression Model of True against Predicted Values")
@

\section{Implement your own lm function}
Write a R function that estimates $\hat{\beta}$ coefficients for the linear model:

\begin{equation}
y = \beta_{0} + \beta_{pred1}x_{1} + \beta_{pred2}x_{2} + \beta_{pred3}x_{3}
\end{equation}

Remember to center scale the predictor variables and estimate $\beta_{0}$ separately. You can 
use the function $scale()$

The coefficients of a linear model can be estimated by

\begin{equation}
\hat{\beta} = (X^{T} X)^{-1} X^{T} y
\end{equation}

\pagebreak
\subsubsection*{Answer}
<<>>=
my.lm <- function(resp, cova){
  errors <- c()
  if (!is.matrix(cova)) errors <- c(errors,  "\ncova must be a matrix")
  if (!is.numeric(cova)) errors <- c(errors, "\ncova must be numeric")
  if (!is.vector(resp)) errors <- c(errors,  "\nresp must be a vector")
  if (!is.numeric(resp)) errors <- c(errors, "\nresp must be numeric")
  if (length(errors) > 0) {
    stop(errors)
  }

  X <- scale(cova,TRUE,TRUE)
  X <- cbind(1,X)

  cp <- t(X) %*% X
  cp.inv <- solve(cp)
  beta <- cp.inv %*% (t(X) %*% resp)
  
  if (!is.null(colnames(cova))) {
    rows <- colnames(cova)
  } else {
    num.coefs <- ncol(cova)
    rows <- paste("beta", 1:num.coefs, sep=".")
  }
  rownames(beta) <- c("Intercept", rows)
  colnames(beta) <- "coefs"

  return(beta)
}
@

\subsubsection*{An example}
<<results=tex>>=
covariates <- matrix(data=c(2,1,5,4,2,3), nrow=3)
colnames(covariates) <- c("x.1", "x.2")
y <- c(16, 8, 19)

my.lm.coefficients <- my.lm(y, covariates)
xtable(my.lm.coefficients)
@

\section{Ridge Regression}

\subsection{Question}
Write a function to estimate the coefficients using a ridge regression model

\begin{equation}
\hat{\beta} = (X^{T} X + \lambda I)^{-1} X^{T} y
\end{equation}

where $I$ is the identity matrix and $\lambda$ the penalty parameter.
\linebreak
\subsubsection*{Answer}

<<>>=
my.lm.ridge <- function(resp, cova, lambda){
  errors <- c()
  if (!is.matrix(cova)) errors <- c(errors,  "\ncova must be a matrix")
  if (!is.numeric(cova)) errors <- c(errors, "\ncova must be numeric")
  if (!is.vector(resp)) errors <- c(errors,  "\nresp must be a vector")
  if (!is.numeric(resp)) errors <- c(errors, "\nresp must be numeric")
  if (!is.numeric(lambda)) errors <- c(errors, "\nlambda must be numeric")
  
  if (length(errors) > 0) {
    stop(errors)
  }

  X <- scale(cova,TRUE,TRUE)
  X <- cbind(1,X)

  cp <- t(X) %*% X
  I <- diag(nrow=nrow(cp), ncol=ncol(cp))
  
  beta <- solve(cp + I*lambda) %*% (t(X) %*% resp)
  
  if (!is.null(colnames(cova))) {
    rows <- colnames(cova)
  } else {
    num.coefs <- ncol(cova)
    rows <- paste("beta", 1:num.coefs, sep=".")
  }
  rownames(beta) <- c("Intercept", rows)
  colnames(beta) <- "coefs"

  return(beta)
}
@

\subsection{Question}
Implement a function that returns the optimal $\lambda$ by a 10 fold cross validation. 
The function minimizes the prediction error measure (sum of squared error)

\begin{equation}
MSE = \sum (y - \hat{y})^{2}
\end{equation}

\verb@rss.error <- sum((test$lpsa-pred)^2)@
\subsubsection*{Answer}

<<>>=
set.seed(131)

find.lambda <- function(resp, cova, lambda.start = 1.0,
                        step.size = 0.1, max.steps = 100) {
  result <- list(
    lambda = lambda.start,
    step.size = step.size,
    steps.to.result = 0
  )
  class(result) <- "find.lambda"

  prev.rss.val <- current.rss.val <- calc.rss(resp, cova, result$lambda)
  prev.lambda <- lambda.start

  repeat {
    result$steps.to.result <- result$steps.to.result + 1
    if (result$steps.to.result > max.steps) {
      stop("Did not converge in maximum number of steps")
    }
    
    # if the rss starts to increase, return lambda
    if (current.rss.val > prev.rss.val) {
      result$lambda <- prev.lambda
      break
    } else {
      prev.lambda <- result$lambda
      result$lambda <- result$lambda + result$step.size
      current.rss.val <- calc.rss(resp, cova, result$lambda)
    }
  }
  
  result
}

#  calculate rss for a given lambda using 10-fold cross validation
calc.rss <- function(resp, cova, lambda, K=10) {
  N <- length(resp)
  random.indices <- sample(x = 1:N, size=N, replace=F)

  #list of vectors with indices of each fold
  folds <- split(random.indices, ceiling(1:N/K))
  
  rss.errors <- sapply(X = folds, FUN=function(test.indices) {
    training.indices <- random.indices[which(!(random.indices %in% test.indices))]
    
    training.cova <- cova[training.indices,]
    training.resp <- resp[training.indices]
    
    test.cova <- cova[test.indices,]
    test.actual.resp <- resp[test.indices]
    
    X <- scale(test.cova)
    X <- cbind(1, X)
    
    beta <- my.lm.ridge(training.resp, training.cova, lambda)
    test.predicted.resp <- (X %*% beta)
    
    # now calculate rss.error
    sum((test.actual.resp - test.predicted.resp)^2)
  })
  mean(rss.errors)
}
@

\subsection{Question}
Apply your function to the prostate dataset and report the model coefficients and optimal 
$\lambda$.

\subsubsection*{Answer}
First, let's find the optimal $\lambda$
<<>>=
training.cova <- as.matrix(subset(prostate.training, select=-lpsa))
training.resp <- as.vector(subset(prostate.training, select=lpsa)[,1])

x <- seq(0.1, 10000, by=0.1)
plot(x, sapply(x, FUN=function(lambda) calc.rss(training.resp, training.cova, lambda = lambda)),
     xlab="lambda", ylab="RSS", log="x")
@

Now that we've discoevered that the optimal value of $\lambda$ is ???????, let's calculate the model coefficients

<<>>=

@

\section{Running the code in this document}
This document was compiled using the \emph{Sweave} extension to R. Sweave is an implementation of the \emph{Literate Programming} paradigm conceived of by Donald Knuth. This allows an author to weave $R$ code and text written in \LaTeX{} together into one document.

To run the code from this document, follow these instructions:

\begin{enumerate}
\item Open the zipped directory containing this pdf \\
  (Alternatively, download the whole project from http://github.com/hiraethus/linear-algebra-assignment)

\item Open RStudio and load this project by choosing \verb@File > Open Project@ and navigating to the file \verb@linear-algebra-assignment.Rproj@.
\item From the file browser in RStudio, choose \verb@linear-algebra-assignment.Rnw@
\item Use the \verb@chunks@ menu located on the text editor opened in R to run each of the chunks in the file
\item Run all the code chunks in RStudio by typing \verb@Ctrl + Alt + R@.
\end{enumerate}

\end{document}
